#!/usr/bin/env python3
"""
Document Analyzer for Competition Round 1B
Enhanced Clean Version - Optimized for Performance
"""

import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=UserWarning)

import os
os.environ['TRANSFORMERS_VERBOSITY'] = 'error'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # Avoid threading issues
os.environ['OMP_NUM_THREADS'] = '1'

import os
import json
import time
import yaml
import re
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Tuple

# Import enhanced modules
from pdf_parser import PDFParser
from nlp_processor import NLPProcessor
from output_generator import OutputGenerator

class CompetitionDocumentAnalyzer:
    """Enhanced Competition Document Analyzer - Optimized for Speed & Accuracy"""
    
    def __init__(self, config_path="config/settings.yaml"):
        self.config_path = config_path
        self.config = self._load_optimized_config()
        
        # Initialize components quietly
        self.pdf_parser = PDFParser()
        self.nlp_processor = NLPProcessor(self.config)
        self.output_generator = OutputGenerator(self.config)
        
        print("üöÄ Document Analyzer Ready")
        
    def _load_optimized_config(self) -> Dict[str, Any]:
        """Load optimized configuration for competition"""
        try:
            if os.path.exists(self.config_path):
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                # Ensure competition optimizations
                config = self._apply_competition_optimizations(config)
            else:
                config = self._get_competition_config()
        except Exception:
            config = self._get_competition_config()
        
        return config
    
    def _apply_competition_optimizations(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Apply competition-specific optimizations to loaded config"""
        # Force competition settings
        config.setdefault('processing', {}).update({
            'max_time_seconds': 55,
            'enable_fast_mode': True,
            'cpu_only': True,
            'batch_size': 16  # Smaller for stability
        })
        
        config.setdefault('model', {}).update({
            'name': 'all-MiniLM-L6-v2',
            'max_length': 384
        })
        
        return config
    
    def _get_competition_config(self) -> Dict[str, Any]:
        """Optimized competition configuration"""
        return {
            'model': {
                'name': 'all-MiniLM-L6-v2',
                'max_length': 384,
                'cache_dir': "./models/",
                'trust_remote_code': False
            },
            'processing': {
                'max_time_seconds': 55,
                'min_section_length': 50,  # Increased for better sections
                'max_sections_per_doc': 15,  # Increased for more content
                'similarity_threshold': 0.45,  # Increased for better relevance
                'enable_fast_mode': True,
                'cpu_only': True,
                'batch_size': 16
            },
            'ranking': {
                'semantic_weight': 0.75,
                'position_weight': 0.15,
                'keyword_weight': 0.10,
                'importance_scale_max': 10,
                'top_sections_count': 8  # Increased for more sections
            },
            'output': {
                'json_indent': 2,
                'include_metadata': True,
                'max_text_length': 500,  # Increased for better content
                'include_scoring_details': True
            },
            'paths': {
                'input_dir': "./input/",
                'output_dir': "./output/",
                'models_dir': "./models/"
            }
        }
    
    def process_documents(self) -> Dict[str, Any]:
        """Main processing pipeline - optimized execution"""
        start_time = time.time()
        
        try:
            # Load inputs with validation
            persona, job, pdf_files = self._load_inputs()
            
            if not pdf_files:
                raise ValueError("No PDF files found")
            
            print(f"üìö Processing {len(pdf_files)} documents...")
            print(f"üë§ Persona: {persona[:50]}...")
            print(f"üéØ Job: {job[:50]}...")
            
            # Parse PDFs with timeout protection
            documents = []
            parse_start = time.time()
            
            for pdf_file in pdf_files:
                if time.time() - start_time > 45:  # Leave time for analysis
                    break
                    
                try:
                    print(f"üìÑ Processing: {pdf_file.name}")
                    doc_data = self.pdf_parser.parse_pdf(str(pdf_file))
                    if doc_data and doc_data.get('sections'):
                        # Filter out very short sections
                        good_sections = [s for s in doc_data['sections'] 
                                       if s.get('word_count', 0) >= 15 and 
                                          len(s.get('content', '')) >= 50]
                        if good_sections:
                            doc_data['sections'] = good_sections
                            documents.append(doc_data)
                            print(f"  ‚úÖ Found {len(good_sections)} quality sections")
                        else:
                            print(f"  ‚ö†Ô∏è No quality sections found")
                except Exception as e:
                    print(f"  ‚ùå Error processing {pdf_file.name}: {e}")
                    continue
            
            if not documents:
                raise ValueError("No documents successfully processed")
            
            print(f"üìÑ Parsed {len(documents)} documents successfully")
            
            # NLP Analysis with time check
            if time.time() - start_time > 50:
                raise TimeoutError("Processing timeout approaching")
                
            print("üß† Analyzing content...")
            analyzed_docs = self.nlp_processor.analyze_documents_dynamically(
                documents, persona, job
            )
            
            # Generate output
            output_data = self.output_generator.generate_output(
                analyzed_docs, persona, job, [f.name for f in pdf_files]
            )
            
            # Save output
            output_path = self._save_output(output_data)
            
            # Calculate metrics
            total_time = time.time() - start_time
            sections_count = len(output_data.get('extracted_sections', []))
            subsections_count = len(output_data.get('subsection_analysis', []))
            
            # Estimate accuracy based on processing quality
            accuracy = self._estimate_accuracy(analyzed_docs, total_time)
            output_data['metadata']['estimated_accuracy'] = accuracy
            
            print(f"‚úÖ Complete: {sections_count} sections, {subsections_count} subsections")
            print(f"üìä Estimated accuracy: {accuracy}% (Time: {total_time:.1f}s)")
            
            return output_data
            
        except Exception as e:
            error_time = time.time() - start_time
            print(f"‚ùå Error after {error_time:.1f}s: {str(e)}")
            
            # Return minimal valid output even on error
            return self._generate_error_output(str(e))
    
    def _load_inputs(self) -> Tuple[str, str, List[Path]]:
        """Load and validate inputs with proper encoding handling"""
        input_dir = Path(self.config['paths']['input_dir'])
        
        # Load persona with fallback and encoding handling
        persona_file = input_dir / "persona.txt"
        if persona_file.exists():
            try:
                persona = self._read_text_file_safely(persona_file)
                print(f"üìù Loaded persona from file: {len(persona)} chars")
            except Exception as e:
                print(f"Warning: Could not read persona.txt: {e}")
                persona = "Expert food contractor specializing in menu planning"
        else:
            persona = "Expert food contractor specializing in menu planning"
            print("üìù Using default persona")
        
        # Load job with fallback and encoding handling
        job_file = input_dir / "job.txt"
        if job_file.exists():
            try:
                job = self._read_text_file_safely(job_file)
                print(f"üéØ Loaded job from file: {len(job)} chars")
            except Exception as e:
                print(f"Warning: Could not read job.txt: {e}")
                job = "Create a comprehensive vegetarian buffet menu with gluten-free options"
        else:
            job = "Create a comprehensive vegetarian buffet menu with gluten-free options"
            print("üéØ Using default job")
        
        # Find PDF files
        pdf_dir = input_dir / "pdfs"
        if not pdf_dir.exists():
            pdf_files = []
            print("‚ùå PDF directory not found")
        else:
            pdf_files = list(pdf_dir.glob("*.pdf"))
            print(f"üìÅ Found {len(pdf_files)} PDF files")
        
        return persona, job, pdf_files
    
    def _read_text_file_safely(self, file_path: Path) -> str:
        """Safely read text files with multiple encoding attempts"""
        # First, check if it's a UTF-16 file with BOM
        try:
            with open(file_path, 'rb') as f:
                raw_data = f.read()
                
            # Check for UTF-16 BOM
            if raw_data.startswith(b'\xff\xfe'):
                print(f"üìù Detected UTF-16 LE BOM in {file_path.name}")
                content = raw_data[2:].decode('utf-16le').strip()
                if content:
                    return content
            elif raw_data.startswith(b'\xfe\xff'):
                print(f"üìù Detected UTF-16 BE BOM in {file_path.name}")
                content = raw_data[2:].decode('utf-16be').strip()
                if content:
                    return content
            elif raw_data.startswith(b'\xef\xbb\xbf'):
                print(f"üìù Detected UTF-8 BOM in {file_path.name}")
                content = raw_data[3:].decode('utf-8').strip()
                if content:
                    return content
        except Exception as e:
            print(f"BOM detection failed for {file_path.name}: {e}")
        
        # Try standard encodings
        encodings = ['utf-8', 'utf-8-sig', 'latin-1', 'cp1252', 'iso-8859-1']
        
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    content = f.read().strip()
                    # Clean up any remaining control characters
                    content = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', content)
                    content = re.sub(r'\s+', ' ', content).strip()
                    if content and len(content) > 3:  # Only return meaningful content
                        print(f"üìù Successfully read {file_path.name} with {encoding}")
                        return content
            except (UnicodeDecodeError, UnicodeError):
                continue
            except Exception as e:
                print(f"Error reading {file_path} with {encoding}: {e}")
                continue
        
        # Final fallback - read as binary and clean
        try:
            with open(file_path, 'rb') as f:
                raw_data = f.read()
                # Try to decode with error handling and clean up
                content = raw_data.decode('utf-8', errors='ignore').strip()
                # Remove control characters and normalize
                content = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', content)
                content = re.sub(r'\s+', ' ', content).strip()
                if content and len(content) > 3:
                    print(f"üìù Binary fallback successful for {file_path.name}")
                    return content
                else:
                    print(f"üìù Trying latin-1 fallback for {file_path.name}")
                    content = raw_data.decode('latin-1', errors='ignore').strip()
                    content = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', content)
                    content = re.sub(r'\s+', ' ', content).strip()
                    return content
        except Exception as e:
            print(f"Final fallback failed for {file_path}: {e}")
            raise ValueError(f"Could not read file {file_path} with any encoding")
    
    def _save_output(self, output_data: Dict[str, Any]) -> Path:
        """Save output with error protection"""
        output_dir = Path(self.config['paths']['output_dir'])
        output_dir.mkdir(parents=True, exist_ok=True)
        
        output_path = output_dir / "challenge1b_output.json"
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, 
                         indent=self.config['output']['json_indent'], 
                         ensure_ascii=False)
            print(f"üíæ Output saved to: {output_path}")
        except Exception as e:
            # Fallback save
            print(f"Warning: UTF-8 save failed, trying fallback: {e}")
            try:
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(output_data, f, indent=2, ensure_ascii=True)
            except Exception as e2:
                print(f"Fallback save also failed: {e2}")
                # Final fallback - basic save
                with open(output_path, 'w') as f:
                    json.dump(output_data, f, indent=2)
        
        return output_path
    
    def _estimate_accuracy(self, analyzed_docs: List[Dict], processing_time: float) -> float:
        """Estimate processing accuracy based on quality indicators"""
        base_accuracy = 75.0
        
        # Time factor (more time usually means better analysis)
        if processing_time > 30:
            time_bonus = 5.0
        elif processing_time > 20:
            time_bonus = 2.0
        else:
            time_bonus = -5.0
        
        # Content quality factor
        if analyzed_docs:
            avg_confidence = sum(doc.get('confidence', 0.5) for doc in analyzed_docs) / len(analyzed_docs)
            confidence_bonus = (avg_confidence - 0.5) * 20  # Convert to percentage
        else:
            confidence_bonus = -10.0
        
        # Document count factor
        doc_count = len(analyzed_docs)
        if 3 <= doc_count <= 7:
            doc_bonus = 5.0
        elif doc_count > 10:
            doc_bonus = -3.0
        else:
            doc_bonus = 0.0
        
        estimated_accuracy = base_accuracy + time_bonus + confidence_bonus + doc_bonus
        return max(60.0, min(95.0, estimated_accuracy))  # Clamp between 60-95%
    
    def _generate_error_output(self, error_message: str) -> Dict[str, Any]:
        """Generate minimal valid output on error"""
        return {
            "metadata": {
                "input_documents": [],
                "persona": "Error in processing",
                "job_to_be_done": "Error in processing", 
                "processing_timestamp": datetime.now().isoformat(),
                "error": error_message,
                "estimated_accuracy": 0.0
            },
            "extracted_sections": [],
            "subsection_analysis": []
        }

def main():
    """Clean main execution with error handling"""
    try:
        analyzer = CompetitionDocumentAnalyzer()
        results = analyzer.process_documents()
        
        # Validate output exists
        output_path = Path("./output/challenge1b_output.json")
        if output_path.exists() and output_path.stat().st_size > 100:
            # Quick quality check
            sections = results.get('extracted_sections', [])
            if len(sections) > 0:
                print(f"‚úÖ Generated {len(sections)} sections successfully")
                return True
            else:
                print("‚ö†Ô∏è No sections generated")
                return False
        else:
            print("‚ùå Output validation failed")
            return False
        
    except Exception as e:
        print(f"‚ùå Critical failure: {str(e)}")
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)